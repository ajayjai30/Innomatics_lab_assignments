{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf2e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data\n",
    "output_path = r'C:\\Users\\admin\\Documents\\Innomatics\\Sentiment\\sentiment_analysis_project\\data\\preprocessed_reviews.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Preprocessed data saved to: {output_path}\")\n",
    "\n",
    "# Display final dataset info\n",
    "print(f\"\\nFinal Dataset Info:\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Positive reviews: {(df['Sentiment']==1).sum()}\")\n",
    "print(f\"Negative reviews: {(df['Sentiment']==0).sum()}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226985b",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56762de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty texts after cleaning\n",
    "empty_texts = (df['cleaned_text'].str.len() == 0).sum()\n",
    "print(f\"Empty texts after cleaning: {empty_texts}\")\n",
    "\n",
    "# Calculate text statistics\n",
    "df['cleaned_length'] = df['cleaned_text'].str.len()\n",
    "df['cleaned_word_count'] = df['cleaned_text'].str.split().str.len()\n",
    "\n",
    "print(\"\\nCleaned Text Statistics:\")\n",
    "print(f\"Average length (characters): {df['cleaned_length'].mean():.2f}\")\n",
    "print(f\"Average word count: {df['cleaned_word_count'].mean():.2f}\")\n",
    "print(f\"Min word count: {df['cleaned_word_count'].min()}\")\n",
    "print(f\"Max word count: {df['cleaned_word_count'].max()}\")\n",
    "\n",
    "# Remove rows with very short cleaned text (optional)\n",
    "print(f\"\\nTexts with less than 3 words: {(df['cleaned_word_count'] < 3).sum()}\")\n",
    "\n",
    "# Keep only texts with at least 3 words\n",
    "df = df[df['cleaned_word_count'] >= 3].reset_index(drop=True)\n",
    "print(f\"Dataset shape after removing very short texts: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c1b9d",
   "metadata": {},
   "source": [
    "## Verify Text Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text cleaning\n",
    "print(\"Applying text cleaning pipeline...\")\n",
    "df = preprocessor.process_dataframe(\n",
    "    df, \n",
    "    text_column='Review text', \n",
    "    output_column='cleaned_text',\n",
    "    remove_stopwords=True,\n",
    "    normalize=True\n",
    ")\n",
    "print(\"Text cleaning completed!\")\n",
    "\n",
    "# Show samples of cleaned text\n",
    "print(\"\\nCLEANED TEXT SAMPLES (after lemmatization and stopword removal):\")\n",
    "print(\"=\"*80)\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df['Review text'].iloc[i][:150]}...\")\n",
    "    print(f\"Cleaned: {df['cleaned_text'].iloc[i][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d295737",
   "metadata": {},
   "source": [
    "## Apply Text Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42764fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the preprocessor with lemmatization\n",
    "preprocessor = TextPreprocessor(use_lemmatization=True)\n",
    "\n",
    "# Show sample of original text\n",
    "print(\"ORIGINAL TEXT SAMPLES:\")\n",
    "print(\"=\"*80)\n",
    "for i in range(3):\n",
    "    print(f\"\\nReview {i+1}:\")\n",
    "    print(df['Review text'].iloc[i][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd12ebe",
   "metadata": {},
   "source": [
    "## Initialize Text Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d426ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values before handling:\")\n",
    "print(df[['Review text', 'Ratings', 'Sentiment']].isnull().sum())\n",
    "\n",
    "# Handle missing values in review text\n",
    "df = handle_missing_values(df, text_column='Review text', strategy='drop')\n",
    "\n",
    "print(f\"\\nDataset shape after handling missing values: {df.shape}\")\n",
    "print(f\"Missing values after handling:\")\n",
    "print(df[['Review text', 'Ratings', 'Sentiment']].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2043c803",
   "metadata": {},
   "source": [
    "## Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba7bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the badminton reviews dataset\n",
    "data_path = r'C:\\Users\\admin\\Documents\\Innomatics\\Sentiment\\reviews_data_dump\\reviews_badminton\\data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Create sentiment labels (ratings >= 3 = positive, < 3 = negative)\n",
    "df['Sentiment'] = (df['Ratings'] >= 3).astype(int)\n",
    "\n",
    "print(f\"Dataset loaded with {len(df)} reviews\")\n",
    "print(f\"Positive reviews: {(df['Sentiment']==1).sum()}\")\n",
    "print(f\"Negative reviews: {(df['Sentiment']==0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5945c59",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd49708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\admin\\Documents\\Innomatics\\Sentiment\\sentiment_analysis_project\\src')\n",
    "\n",
    "from preprocessing import TextPreprocessor, create_binary_labels, handle_missing_values\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c8384",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing\n",
    "## Cleaning and Normalizing Review Text\n",
    "\n",
    "In this notebook, we will:\n",
    "- Load cleaned EDA data\n",
    "- Remove special characters and punctuation\n",
    "- Remove HTML tags and URLs\n",
    "- Normalize text (lowercase, whitespace)\n",
    "- Remove stopwords\n",
    "- Apply lemmatization\n",
    "- Handle missing values"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
